{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"b5c0d2440b3e4995a794ded565213150","deepnote_cell_type":"markdown"},"source":["<h1><center>Laboratorio 9: Optimización de modelos 💯</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos</strong></center>"]},{"cell_type":"markdown","metadata":{"cell_id":"bfb94b9656f145ad83e81b75d218cb70","deepnote_cell_type":"markdown"},"source":["### Cuerpo Docente:\n","\n","- Profesor: Ignacio Meza, Gabriel Iturra\n","- Auxiliar: Sebastián Tinoco\n","- Ayudante: Arturo Lazcano, Angelo Muñoz"]},{"cell_type":"markdown","metadata":{"cell_id":"b1b537fdd27c43909a49d3476ce64d91","deepnote_cell_type":"markdown"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n","\n","- Nombre de alumno 1: Tomás Aguirre\n","- Nombre de alumno 2: Ignacio Albornoz\n"]},{"cell_type":"markdown","metadata":{"cell_id":"b7dbdd30ab544cb8a8afe00648a586ae","deepnote_cell_type":"markdown"},"source":["## Temas a tratar\n","\n","- Predicción de demanda usando `xgboost`\n","- Búsqueda del modelo óptimo de clasificación usando `optuna`\n","- Uso de pipelines.\n","\n","## Reglas:\n","\n","- **Grupos de 2 personas**\n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n","- Prohibidas las copias. \n","- Pueden usar cualquer material del curso que estimen conveniente.\n","\n","### Objetivos principales del laboratorio\n","\n","- Optimizar modelos usando `optuna`\n","- Recurrir a técnicas de *prunning*\n","- Forzar el aprendizaje de relaciones entre variables mediante *constraints*\n","- Fijar un pipeline con un modelo base que luego se irá optimizando.\n","\n","El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."]},{"cell_type":"markdown","metadata":{"cell_id":"f38c8342f5164aa992a97488dd5590bf","deepnote_cell_type":"markdown"},"source":["### **Link de repositorio de GitHub:** `https://github.com/tomasaguirre-ignacioalbornoz/MDS7202`"]},{"cell_type":"markdown","metadata":{"cell_id":"f1c73babb7f74af588a4fa6ae14829e0","deepnote_cell_type":"markdown"},"source":["# Importamos librerias útiles"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"51afe4d2df42442b9e5402ffece60ead","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4957,"execution_start":1699544354044,"source_hash":null},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: joblib in /home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages (1.3.2)\n"]}],"source":["!pip install -qq xgboost optuna\n","!pip install joblib"]},{"cell_type":"markdown","metadata":{"cell_id":"44d227389a734ac59189c5e0005bc68a","deepnote_cell_type":"markdown"},"source":["# 1. El emprendimiento de Fiu\n","\n","Tras liderar de manera exitosa la implementación de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corpóreo **Fiu** se anima y decide levantar su propio negocio de consultoría en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Como usted tuvo un rendimiento sobresaliente en el proyecto de caracterización de datos, Fiu lo contrata como *data scientist* de su emprendimiento.\n","\n","Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n","\n","Para comenzar, cargue el dataset señalado y visualice a través de un `.head` los atributos que posee el dataset.\n","\n","<i><p align=\"center\">Fiu siendo felicitado por su excelente desempeño en el proyecto de caracterización de datos</p></i>\n","<p align=\"center\">\n","  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n","</p>"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"2f9c82d204b14515ad27ae07e0b77702","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":92,"execution_start":1699544359006,"source_hash":null},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>city</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>pop</th>\n","      <th>shop</th>\n","      <th>brand</th>\n","      <th>container</th>\n","      <th>capacity</th>\n","      <th>price</th>\n","      <th>quantity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>glass</td>\n","      <td>500ml</td>\n","      <td>0.96</td>\n","      <td>13280</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>plastic</td>\n","      <td>1.5lt</td>\n","      <td>2.86</td>\n","      <td>6727</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.87</td>\n","      <td>9848</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>adult-cola</td>\n","      <td>glass</td>\n","      <td>500ml</td>\n","      <td>1.00</td>\n","      <td>20050</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>adult-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.39</td>\n","      <td>25696</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id       date    city       lat      long     pop    shop        brand  \\\n","0   0 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","1   1 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","2   2 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","3   3 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n","4   4 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n","\n","  container capacity  price  quantity  \n","0     glass    500ml   0.96     13280  \n","1   plastic    1.5lt   2.86      6727  \n","2       can    330ml   0.87      9848  \n","3     glass    500ml   1.00     20050  \n","4       can    330ml   0.39     25696  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","df = pd.read_csv('sales.csv')\n","df['date'] = pd.to_datetime(df['date'], format='%d/%m/%y')\n","\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"cell_id":"b50db6f2cb804932ae3f9e5748a6ea61","deepnote_cell_type":"markdown"},"source":["## 1.1 Generando un Baseline (0.5 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n","</p>\n","\n","Antes de entrenar un algoritmo, usted recuerda los apuntes de su magíster en ciencia de datos y recuerda que debe seguir una serie de *buenas prácticas* para entrenar correcta y debidamente su modelo. Después de un par de vueltas, llega a las siguientes tareas:\n","\n","1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad.\n","2. Implemente un `FunctionTransformer` para extraer el día, mes y año de la variable `date`. Guarde estas variables en el formato categorical de pandas.\n","3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos numéricos y categóricos. Use `OneHotEncoder` para las variables categóricas.\n","4. Guarde los pasos anteriores en un `Pipeline`, dejando como último paso el regresor `DummyRegressor` para generar predicciones en base a promedios.\n","5. Entrene el pipeline anterior y reporte la métrica `mean_absolute_error` sobre los datos de validación. ¿Cómo se interpreta esta métrica para el contexto del negocio?\n","6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los parámetros por default**. ¿Cómo cambia el MAE al implementar este algoritmo? ¿Es mejor o peor que el `DummyRegressor`?\n","7. Guarde ambos modelos en un archivo .pkl (uno cada uno)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE con DummyRegressor: 13298.497767341096\n","Features after preprocessing: ['price', 'id', 'lat', 'long', 'pop', 'day_28', 'day_29', 'day_30', 'day_31', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016', 'year_2017', 'year_2018', 'city_Athens', 'city_Irakleion', 'city_Larisa', 'city_Patra', 'city_Thessaloniki', 'shop_shop_1', 'shop_shop_2', 'shop_shop_3', 'shop_shop_4', 'shop_shop_5', 'shop_shop_6', 'brand_adult-cola', 'brand_gazoza', 'brand_kinder-cola', 'brand_lemon-boost', 'brand_orange-power', 'container_can', 'container_glass', 'container_plastic', 'capacity_1.5lt', 'capacity_330ml', 'capacity_500ml']\n","MAE con XGBRegressor: 2424.366823499591\n"]},{"data":{"text/plain":["['model_xgb.pkl']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Importar librerías necesarias\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.dummy import DummyRegressor\n","from sklearn.metrics import mean_absolute_error\n","from xgboost import XGBRegressor\n","import joblib\n","\n","\n","ordered_columns = ['price'] + [col for col in df.columns if col != 'price']\n","df = df[ordered_columns]\n","\n","# Crear un FunctionTransformer para extraer día, mes y año\n","def extract_date_parts(df):\n","    df['day'] = df['date'].dt.day.astype('category')\n","    df['month'] = df['date'].dt.month.astype('category')\n","    df['year'] = df['date'].dt.year.astype('category')\n","    df = df.drop(columns=['date'])\n","    # Crear una lista con 'price' como el primer elemento seguido por el resto de las columnas\n","    #ordered_columns = ['price'] + [col for col in df.columns if col != 'price']\n","    #df = df[ordered_columns]\n","    return df\n","\n","\n","\n","\n","\n","\n","def get_feature_names(column_transformer):\n","    \"\"\"Get feature names from a ColumnTransformer.\"\"\"\n","    output_features = []\n","\n","    for name, pipe, features in column_transformer.transformers_:\n","        # Process each transformer\n","        if name != 'remainder':\n","            if hasattr(pipe, 'get_feature_names_out'):\n","                # If the transformer has a get_feature_names_out method, use it\n","                feature_names = pipe.get_feature_names_out(features)\n","                output_features.extend(feature_names)\n","            else:\n","                # Otherwise, just append the feature names as is\n","                output_features.extend(features)\n","        else:\n","            # If the remainder transformer is used, handle accordingly\n","            remainder_features = [f for f in features if f not in output_features]\n","            output_features.extend(remainder_features)\n","\n","    return output_features\n","\n","\n","\n","# Separar en conjuntos de train, validation y test\n","train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=(1/3), random_state=42)\n","\n","\n","\n","#print(extract_date_parts(df).head())\n","\n","date_transformer = FunctionTransformer(extract_date_parts, validate=False)\n","\n","# Crear un ColumnTransformer para procesar los datos\n","#numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n","\n","numeric_features = df.select_dtypes(include=['int64', 'float64']).drop('quantity', axis=1).columns\n","\n","\n","# Lista inicial de características categóricas\n","categorical_features = ['day', 'month', 'year']\n","\n","# Añadir columnas del DataFrame que no son de tipo int64 o float64\n","categorical_features.extend(df.select_dtypes(exclude=['int64', 'float64']).columns)\n","\n","# Excluir la columna 'date'\n","categorical_features.remove('date')\n","\n","\n","'''\n","print(\"numeric_features\")\n","print(numeric_features)\n","print(\"categorical_features\")\n","print(categorical_features)\n","'''\n","# Creando un dataframe que solo contiene la columna 'quantity'\n","train_Y = train_df[['quantity']]\n","\n","# Creando otro dataframe que contiene todas las columnas excepto 'quantity'\n","train_X = train_df.drop(columns=['quantity'])\n","\n","#print(train_X.columns)\n","\n","# Creando un dataframe que solo contiene la columna 'quantity'\n","test_Y = test_df[['quantity']]\n","\n","# Creando otro dataframe que contiene todas las columnas excepto 'quantity'\n","test_X = test_df.drop(columns=['quantity'])\n","\n","# Creando un dataframe que solo contiene la columna 'quantity'\n","val_Y= val_df[['quantity']]\n","\n","# Creando otro dataframe que contiene todas las columnas excepto 'quantity'\n","val_X = val_df.drop(columns=['quantity'])\n","\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', MinMaxScaler(), numeric_features),\n","        ('cat', OneHotEncoder(sparse_output=False), categorical_features)\n","    ])\n","\n","'''\n","# Fit and transform the data\n","df_eda = preprocessor.fit_transform(df)\n","\n","# Extract feature names for categorical features transformed by OneHotEncoder\n","# If 'cat' is the name given to the OneHotEncoder step in your ColumnTransformer\n","cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out()\n","\n","print(cat_feature_names)\n","\n","# Concatenate all feature names (numeric + categorical)\n","all_feature_names = numeric_features + list(cat_feature_names)\n","\n","# Create a DataFrame with the new feature names\n","df_eda = pd.DataFrame(df_eda, columns=all_feature_names)\n","\n","# Now you can use the corr() method\n","print(df_eda.corr())\n","'''\n","\n","# Crear y entrenar el pipeline con DummyRegressor\n","pipeline_dummy = Pipeline(steps=[\n","    ('date', date_transformer),\n","    ('preprocessor', preprocessor),\n","    ('regressor', DummyRegressor(strategy='mean'))\n","])\n","\n","\n","#print(\"debug1\")\n","pipeline_dummy.fit(train_X, train_Y) \n","\n","# Evaluar el modelo\n","y_pred = pipeline_dummy.predict(val_X)\n","\n","mae_dummy = mean_absolute_error(val_Y, y_pred)\n","print(f'MAE con DummyRegressor: {mae_dummy}')\n","\n","\n","# After fitting your pipeline, call this function\n","feature_names_after_preprocessing = get_feature_names(pipeline_dummy.named_steps['preprocessor'])\n","print(\"Features after preprocessing:\", feature_names_after_preprocessing)\n","\n","\n","# Reemplazar DummyRegressor con XGBRegressor y entrenar nuevamente\n","pipeline_xgb = Pipeline(steps=[\n","    ('date', date_transformer),\n","    ('preprocessor', preprocessor),\n","    ('regressor', XGBRegressor())\n","])\n","\n","pipeline_xgb.fit(train_X, train_Y) \n","\n","# Evaluar el nuevo modelo\n","y_pred_xgb = pipeline_xgb.predict(val_X)\n","mae_xgb = mean_absolute_error(val_Y, y_pred_xgb)\n","print(f'MAE con XGBRegressor: {mae_xgb}')\n","\n","# Guardar los modelos\n","joblib.dump(pipeline_dummy, 'model_dummy.pkl')\n","joblib.dump(pipeline_xgb, 'model_xgb.pkl')\n"]},{"cell_type":"markdown","metadata":{"cell_id":"7e17e46063774ec28226fe300d42ffe0","deepnote_cell_type":"markdown"},"source":["## 1.2 Forzando relaciones entre parámetros con XGBoost (1.0 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n","</p>\n","\n","Un colega aficionado a la economía le *sopla* que la demanda guarda una relación inversa con el precio del producto. Motivado para impresionar al querido corpóreo, se propone hacer uso de esta información para mejorar su modelo.\n","\n","Vuelva a entrenar el `Pipeline`, pero esta vez forzando una relación monótona negativa entre el precio y la cantidad. Luego, vuelva a reportar el `MAE` sobre el conjunto de validación. ¿Cómo cambia el error al incluir esta relación? ¿Tenía razón su amigo?\n","\n","Nuevamente, guarde su modelo en un archivo .pkl\n","\n","Nota: Para realizar esta parte, debe apoyarse en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentación</a>.\n","\n","Hint: Para implementar el constraint, se le sugiere hacerlo especificando el nombre de la variable. De ser así, probablemente le sea útil **mantener el formato de pandas** antes del step de entrenamiento."]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"f469f3b572be434191d2d5c3f11b20d2","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE con XGBRegressor y constraint monótono: 2500.521823322749\n"]},{"data":{"text/plain":["['model_xgb_monotone.pkl']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Definir los constraints de monotonía\n","# Asumiendo que 'price' es la primera columna después del preprocesamiento\n","# -1 indica una relación monótona negativa\n","monotone_constraints = (-1,)\n","\n","# Incluir el inspector de datos en el pipeline antes del regresor\n","pipeline_xgb_monotone = Pipeline(steps=[\n","    ('date', date_transformer),\n","    ('preprocessor', preprocessor),\n","    ('regressor', XGBRegressor(monotone_constraints=monotone_constraints))\n","])\n","\n","pipeline_xgb_monotone.fit(train_X, train_Y)\n","\n","# Evaluar el modelo\n","y_pred_xgb_monotone = pipeline_xgb_monotone.predict(val_X)\n","mae_xgb_monotone = mean_absolute_error(val_Y, y_pred_xgb_monotone)\n","print(f'MAE con XGBRegressor y constraint monótono: {mae_xgb_monotone}')\n","\n","# Guardar el modelo\n","joblib.dump(pipeline_xgb_monotone, 'model_xgb_monotone.pkl')\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"e59ef80ed20b4de8921f24da74e87374","deepnote_cell_type":"markdown"},"source":["## 1.3 Optimización de Hiperparámetros con Optuna (2.0 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n","</p>\n","\n","Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun más* su modelo. En particular, le comenta de la optimización de hiperparámetros con metodologías bayesianas a través del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n","\n","A partir de la mejor configuración obtenida en la sección anterior, utilice `optuna` para optimizar sus hiperparámetros. En particular, se le pide:\n","\n","- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n","- Utilice `TPESampler` como método de muestreo\n","- De `XGBRegressor`, optimice los siguientes hiperparámetros:\n","    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n","    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n","    - `max_depth` buscando valores enteros en el rango (3, 10)\n","    - `max_leaves` buscando valores enteros en el rango (0, 100)\n","    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n","    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n","    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n","- De `OneHotEncoder`, optimice el hiperparámetro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n","- Explique cada hiperparámetro y su rol en el modelo. ¿Hacen sentido los rangos de optimización indicados?\n","- Fije el tiempo de entrenamiento a 5 minutos\n","- Reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto?\n","- Guardar su modelo en un archivo .pkl"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"de5914621cc64cb0b1bacb9ff565a97e","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["Número de trials: 100\n","Mejores hiperparámetros: {'learning_rate': 0.07030656263631437, 'n_estimators': 829, 'max_depth': 9, 'max_leaves': 89, 'min_child_weight': 3, 'reg_alpha': 0.07298575769548347, 'reg_lambda': 0.7014716927898965, 'min_frequency': 0.6799054775385127}\n","MAE óptimo: 2034.7008926934\n"]}],"source":["import optuna\n","\n","# Define la función objetivo para Optuna\n","def objective(trial):\n","    # Definir los rangos de búsqueda para los hiperparámetros\n","    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n","    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n","    max_depth = trial.suggest_int('max_depth', 3, 10)\n","    max_leaves = trial.suggest_int('max_leaves', 0, 100)\n","    min_child_weight = trial.suggest_int('min_child_weight', 1, 5)\n","    reg_alpha = trial.suggest_float('reg_alpha', 0, 1)\n","    reg_lambda = trial.suggest_float('reg_lambda', 0, 1)\n","    \n","    min_frequency = trial.suggest_float('min_frequency', 0.0, 1.0)\n","\n","    # Crear un nuevo pipeline con los hiperparámetros sugeridos por Optuna\n","    xgb_model = XGBRegressor(\n","        learning_rate=learning_rate,\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        max_leaves=max_leaves,\n","        min_child_weight=min_child_weight,\n","        reg_alpha=reg_alpha,\n","        reg_lambda=reg_lambda\n","    )\n","\n","    # Modificar el valor de min_frequency del OneHotEncoder en el ColumnTransformer\n","    for name, transformer, columns in preprocessor.transformers_:\n","        if isinstance(transformer, OneHotEncoder):\n","            transformer.set_params(min_frequency=min_frequency)\n","\n","    pipeline_xgb = Pipeline(steps=[\n","        ('date', date_transformer),\n","        ('preprocessor', preprocessor),\n","        ('regressor', xgb_model)\n","    ])\n","\n","    pipeline_xgb.fit(train_X, train_Y)\n","\n","    # Calcular MAE en datos de validación\n","    y_pred = pipeline_xgb.predict(val_X)\n","    mae = mean_absolute_error(val_Y, y_pred)\n","\n","    return mae\n","\n","# Crear un estudio de Optuna\n","study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=314159))\n","study.optimize(objective, n_trials=100, timeout=300)\n","\n","# Obtener los mejores hiperparámetros encontrados\n","best_params = study.best_params\n","mae_optuna_1 = study.best_value\n","num_trials = len(study.trials)\n","\n","print(\"Número de trials:\", num_trials)\n","print(\"Mejores hiperparámetros:\", best_params)\n","print(\"MAE óptimo:\", mae_optuna_1)"]},{"cell_type":"markdown","metadata":{"cell_id":"5195ccfc37e044ad9453f6eb2754f631","deepnote_cell_type":"markdown"},"source":["## 1.4 Optimización de Hiperparámetros con Optuna y Prunners (1.7)\n","\n","<p align=\"center\">\n","  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n","</p>\n","\n","Después de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en sí mismo. Después de leer un par de post de personas de dudosa reputación en la *deepweb*, usted llega a la conclusión que puede cumplir este objetivo mediante la implementación de **Prunning**.\n","\n","Vuelva a optimizar los mismos hiperparámetros que la sección pasada, pero esta vez utilizando **Prunning** en la optimización. En particular, usted debe:\n","\n","- Responder: ¿Qué es prunning? ¿De qué forma debería impactar en el entrenamiento?\n","- Utilizar `optuna.integration.XGBoostPruningCallback` como método de **Prunning**\n","- Fijar nuevamente el tiempo de entrenamiento a 5 minutos\n","- Reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto?\n","- Guardar su modelo en un archivo .pkl\n","\n","Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n","\n","```\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","```\n","\n","De implementar la opción anterior, pueden especificar `show_progress_bar = True` en el método `optimize` para *más sabor*.\n","\n","Hint: Si quieren especificar parámetros del método .fit() del modelo a través del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n","\n","Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementación"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"eeaa967cd8f6426d8c54f276c17dce79","deepnote_cell_type":"code"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n","  warnings.warn(\n","/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `callbacks` in `fit` method is deprecated for better compatibility with scikit-learn, use `callbacks` in constructor or`set_params` instead.\n","  warnings.warn(\n","[W 2023-11-15 21:02:52,485] Trial 0 failed with parameters: {'learning_rate': 0.08197440749175473, 'n_estimators': 574, 'max_depth': 6, 'max_leaves': 9, 'min_child_weight': 5, 'reg_alpha': 0.9673564038996015, 'reg_lambda': 0.09820669376309132, 'min_frequency': 0.8018603712796477} because of the following error: KeyError('validation_0-mae').\n","Traceback (most recent call last):\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","                      ^^^^^^^^^^^\n","  File \"/tmp/ipykernel_88181/2405789120.py\", line 65, in objective_with_prunning\n","    xgb_model.fit(\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n","    return func(**kwargs)\n","           ^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1086, in fit\n","    self._Booster = train(\n","                    ^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n","    return func(**kwargs)\n","           ^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/training.py\", line 182, in train\n","    if cb_container.after_iteration(bst, i, dtrain, evals):\n","       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py\", line 241, in after_iteration\n","    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py\", line 241, in <genexpr>\n","    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/integration/xgboost.py\", line 71, in after_iteration\n","    current_score = evaluation_results[self._observation_key]\n","                    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n","KeyError: 'validation_0-mae'\n","[W 2023-11-15 21:02:52,488] Trial 0 failed with value None.\n"]},{"ename":"KeyError","evalue":"'validation_0-mae'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mae\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m study_with_prunning \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m314159\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m study_with_prunning\u001b[39m.\u001b[39;49moptimize(objective_with_prunning, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Obtener los mejores hiperparámetros encontrados con prunning\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m best_params_with_prunning \u001b[39m=\u001b[39m study_with_prunning\u001b[39m.\u001b[39mbest_params\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n","\u001b[1;32m/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mColumnas entregadas a XGBoost:\u001b[39m\u001b[39m\"\u001b[39m, train_X_processed\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Entrenar el modelo con los datos procesados\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m xgb_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     train_X_processed, train_Y, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(val_X_processed, val_Y)], \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[XGBoostPruningCallback(trial, \u001b[39m\"\u001b[39;49m\u001b[39mvalidation_0-mae\u001b[39;49m\u001b[39m\"\u001b[39;49m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Calcular MAE en datos de validación\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m y_pred \u001b[39m=\u001b[39m xgb_model\u001b[39m.\u001b[39mpredict(val_X_processed)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m (\n\u001b[1;32m   1078\u001b[0m     model,\n\u001b[1;32m   1079\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1085\u001b[0m )\n\u001b[0;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1087\u001b[0m     params,\n\u001b[1;32m   1088\u001b[0m     train_dmatrix,\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39;49mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    185\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39mafter_training(bst)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[39m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39mafter_iteration(model, epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[39m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39;49mafter_iteration(model, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/integration/xgboost.py:71\u001b[0m, in \u001b[0;36mXGBoostPruningCallback.after_iteration\u001b[0;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m             evaluation_results[key] \u001b[39m=\u001b[39m scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 71\u001b[0m current_score \u001b[39m=\u001b[39m evaluation_results[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_observation_key]\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trial\u001b[39m.\u001b[39mreport(current_score, step\u001b[39m=\u001b[39mepoch)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trial\u001b[39m.\u001b[39mshould_prune():\n","\u001b[0;31mKeyError\u001b[0m: 'validation_0-mae'"]}],"source":["import optuna\n","from optuna.integration import XGBoostPruningCallback\n","from sklearn.base import BaseEstimator, TransformerMixin\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","\n","\n","def objective_with_prunning(trial):\n","    # Definir los rangos de búsqueda para los hiperparámetros\n","    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n","    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n","    max_depth = trial.suggest_int('max_depth', 3, 10)\n","    max_leaves = trial.suggest_int('max_leaves', 0, 100)\n","    min_child_weight = trial.suggest_int('min_child_weight', 1, 5)\n","    reg_alpha = trial.suggest_float('reg_alpha', 0, 1)\n","    reg_lambda = trial.suggest_float('reg_lambda', 0, 1)\n","    min_frequency = trial.suggest_float('min_frequency', 0.0, 1.0)\n","\n","    # Crear el modelo XGBoost con los hiperparámetros sugeridos\n","    xgb_model = XGBRegressor(\n","        learning_rate=learning_rate,\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        max_leaves=max_leaves,\n","        min_child_weight=min_child_weight,\n","        reg_alpha=reg_alpha,\n","        reg_lambda=reg_lambda,\n","        random_state=42  # Para garantizar la reproducibilidad\n","    )\n","\n","    '''\n","    # Modificar OneHotEncoder en el preprocesador\n","    for name, transformer, columns in preprocessor.transformers_:\n","        if isinstance(transformer, OneHotEncoder):\n","            transformer.set_params(min_frequency=min_frequency)\n","\n","    '''\n","\n","    class ColumnInspector(BaseEstimator, TransformerMixin):\n","        def fit(self, X, y=None):\n","            return self\n","\n","        def transform(self, X):\n","            # Si X es un DataFrame, imprime las columnas\n","            if hasattr(X, 'columns'):\n","                print(\"Columnas entregadas a XGBoost:\", X.columns)\n","            # Si no, asume que es un array numpy y no hace nada especial\n","            return X\n","\n","    # Crear el pipeline\n","    pipeline_xgb = Pipeline(steps=[\n","        ('preprocessor', preprocessor),\n","        ('regressor', xgb_model)\n","    ])\n","\n","    # Aquí aplicamos el preprocesador manualmente para inspeccionar los datos\n","    train_X_processed = preprocessor.fit_transform(train_X)\n","    val_X_processed = preprocessor.transform(val_X)\n","\n","    # Imprimir las columnas después del preprocesamiento\n","    # Esto asume que el resultado es un DataFrame, si no lo es, ajustar según sea necesario\n","    if hasattr(train_X_processed, 'columns'):\n","        print(\"Columnas entregadas a XGBoost:\", train_X_processed.columns)\n","\n","    # Entrenar el modelo con los datos procesados\n","    xgb_model.fit(\n","        train_X_processed, train_Y, \n","        eval_set=[(val_X_processed, val_Y)], \n","        early_stopping_rounds=100, \n","        callbacks=[XGBoostPruningCallback(trial, \"validation_0-mae\")]\n","    )\n","\n","    # Calcular MAE en datos de validación\n","    y_pred = xgb_model.predict(val_X_processed)\n","    mae = mean_absolute_error(val_Y, y_pred)\n","\n","    return mae\n","\n","\n","study_with_prunning = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=314159))\n","study_with_prunning.optimize(objective_with_prunning, n_trials=100, timeout=300)\n","\n","# Obtener los mejores hiperparámetros encontrados con prunning\n","best_params_with_prunning = study_with_prunning.best_params\n","mae_optuna_with_prunning = study_with_prunning.best_value\n","num_trials_with_prunning = len(study_with_prunning.trials)\n","\n","print(\"Número de trials con prunning:\", num_trials_with_prunning)\n","print(\"Mejores hiperparámetros con prunning:\", best_params_with_prunning)\n","print(\"MAE óptimo con prunning:\", mae_optuna_with_prunning)"]},{"cell_type":"markdown","metadata":{"cell_id":"8a081778cc704fc6bed05393a5419327","deepnote_cell_type":"markdown"},"source":["## 1.5 Visualizaciones (0.5 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n","</p>\n","\n","\n","Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n","\n","A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n","\n","- Gráfico de historial de optimización\n","- Gráfico de coordenadas paralelas\n","- Gráfico de importancia de hiperparámetros\n","\n","Comente sus resultados: ¿Desde qué *trial* se empiezan a observar mejoras notables en sus resultados? ¿Qué tendencias puede observar a partir del gráfico de coordenadas paralelas? ¿Cuáles son los hiperparámetros con mayor importancia para la optimización de su modelo?"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"0e706dc9a8d946eda7a9eb1f0463c6d7","deepnote_cell_type":"code"},"outputs":[],"source":["# Inserte su código acá"]},{"cell_type":"markdown","metadata":{"cell_id":"ac8a20f445d045a3becf1a518d410a7d","deepnote_cell_type":"markdown"},"source":["## 1.6 Síntesis de resultados (0.3)\n","\n","Finalmente, genere una tabla resumen del MAE obtenido en los 5 modelos entrenados (desde Baseline hasta XGBoost con Constraints, Optuna y Prunning) y compare sus resultados. ¿Qué modelo obtiene el mejor rendimiento? \n","\n","Por último, cargue el mejor modelo, prediga sobre el conjunto de test y reporte su MAE. ¿Existen diferencias con respecto a las métricas obtenidas en el conjunto de validación? ¿Porqué puede ocurrir esto?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calcula el MAE para cada modelo\n","mae_baseline = mae_dummy\n","mae_xgboost = mae_xgb  \n","mae_optuna = mae_optuna_1\n","mae_constraints = mae_xgb_monotone \n","mae_pruning = 3.5  \n","\n","# Crea un DataFrame con los valores de MAE\n","data = {\n","    'Modelo': ['Baseline', 'XGBoost', 'XGBoost (Optuna)', 'XGBoost (Constraints)', 'XGBoost (Optuna, Pruning)'],\n","    'MAE': [mae_baseline, mae_xgboost, mae_optuna, mae_constraints, mae_pruning]\n","}\n","\n","tabla = pd.DataFrame(data)\n","\n","# Imprime la tabla resumen\n","display(tabla)"]},{"cell_type":"markdown","metadata":{"cell_id":"5c4654d12037494fbd385b4dc6bd1059","deepnote_cell_type":"markdown"},"source":["# Conclusión\n","Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/8CT1AXElF_cAAAAC/gojo-satoru.gif\">\n","</p>"]},{"cell_type":"markdown","metadata":{"cell_id":"5025de06759f4903a26916c80323bf25","deepnote_cell_type":"markdown"},"source":[]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"f63d38450a6b464c9bb6385cf11db4d9","deepnote_persisted_session":{"createdAt":"2023-11-09T16:18:30.203Z"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
