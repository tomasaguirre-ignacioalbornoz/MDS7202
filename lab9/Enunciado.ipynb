{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"b5c0d2440b3e4995a794ded565213150","deepnote_cell_type":"markdown"},"source":["<h1><center>Laboratorio 9: Optimizaci칩n de modelos 游눮</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos</strong></center>"]},{"cell_type":"markdown","metadata":{"cell_id":"bfb94b9656f145ad83e81b75d218cb70","deepnote_cell_type":"markdown"},"source":["### Cuerpo Docente:\n","\n","- Profesor: Ignacio Meza, Gabriel Iturra\n","- Auxiliar: Sebasti치n Tinoco\n","- Ayudante: Arturo Lazcano, Angelo Mu침oz"]},{"cell_type":"markdown","metadata":{"cell_id":"b1b537fdd27c43909a49d3476ce64d91","deepnote_cell_type":"markdown"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados\n","\n","- Nombre de alumno 1: Tom치s Aguirre\n","- Nombre de alumno 2: Ignacio Albornoz\n"]},{"cell_type":"markdown","metadata":{"cell_id":"b7dbdd30ab544cb8a8afe00648a586ae","deepnote_cell_type":"markdown"},"source":["## Temas a tratar\n","\n","- Predicci칩n de demanda usando `xgboost`\n","- B칰squeda del modelo 칩ptimo de clasificaci칩n usando `optuna`\n","- Uso de pipelines.\n","\n","## Reglas:\n","\n","- **Grupos de 2 personas**\n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n","- Prohibidas las copias. \n","- Pueden usar cualquer material del curso que estimen conveniente.\n","\n","### Objetivos principales del laboratorio\n","\n","- Optimizar modelos usando `optuna`\n","- Recurrir a t칠cnicas de *prunning*\n","- Forzar el aprendizaje de relaciones entre variables mediante *constraints*\n","- Fijar un pipeline con un modelo base que luego se ir치 optimizando.\n","\n","El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."]},{"cell_type":"markdown","metadata":{"cell_id":"f38c8342f5164aa992a97488dd5590bf","deepnote_cell_type":"markdown"},"source":["### **Link de repositorio de GitHub:** `https://github.com/tomasaguirre-ignacioalbornoz/MDS7202`"]},{"cell_type":"markdown","metadata":{"cell_id":"f1c73babb7f74af588a4fa6ae14829e0","deepnote_cell_type":"markdown"},"source":["# Importamos librerias 칰tiles"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"51afe4d2df42442b9e5402ffece60ead","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4957,"execution_start":1699544354044,"source_hash":null},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: joblib in /home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages (1.3.2)\n"]}],"source":["!pip install -qq xgboost optuna\n","!pip install joblib"]},{"cell_type":"markdown","metadata":{"cell_id":"44d227389a734ac59189c5e0005bc68a","deepnote_cell_type":"markdown"},"source":["# 1. El emprendimiento de Fiu\n","\n","Tras liderar de manera exitosa la implementaci칩n de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corp칩reo **Fiu** se anima y decide levantar su propio negocio de consultor칤a en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Como usted tuvo un rendimiento sobresaliente en el proyecto de caracterizaci칩n de datos, Fiu lo contrata como *data scientist* de su emprendimiento.\n","\n","Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n","\n","Para comenzar, cargue el dataset se침alado y visualice a trav칠s de un `.head` los atributos que posee el dataset.\n","\n","<i><p align=\"center\">Fiu siendo felicitado por su excelente desempe침o en el proyecto de caracterizaci칩n de datos</p></i>\n","<p align=\"center\">\n","  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n","</p>"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"2f9c82d204b14515ad27ae07e0b77702","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":92,"execution_start":1699544359006,"source_hash":null},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>city</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>pop</th>\n","      <th>shop</th>\n","      <th>brand</th>\n","      <th>container</th>\n","      <th>capacity</th>\n","      <th>price</th>\n","      <th>quantity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>glass</td>\n","      <td>500ml</td>\n","      <td>0.96</td>\n","      <td>13280</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>plastic</td>\n","      <td>1.5lt</td>\n","      <td>2.86</td>\n","      <td>6727</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>kinder-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.87</td>\n","      <td>9848</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>adult-cola</td>\n","      <td>glass</td>\n","      <td>500ml</td>\n","      <td>1.00</td>\n","      <td>20050</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2012-01-31</td>\n","      <td>Athens</td>\n","      <td>37.97945</td>\n","      <td>23.71622</td>\n","      <td>672130</td>\n","      <td>shop_1</td>\n","      <td>adult-cola</td>\n","      <td>can</td>\n","      <td>330ml</td>\n","      <td>0.39</td>\n","      <td>25696</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id       date    city       lat      long     pop    shop        brand  \\\n","0   0 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","1   1 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","2   2 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n","3   3 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n","4   4 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n","\n","  container capacity  price  quantity  \n","0     glass    500ml   0.96     13280  \n","1   plastic    1.5lt   2.86      6727  \n","2       can    330ml   0.87      9848  \n","3     glass    500ml   1.00     20050  \n","4       can    330ml   0.39     25696  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","\n","df = pd.read_csv('sales.csv')\n","df['date'] = pd.to_datetime(df['date'], format='%d/%m/%y')\n","\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"cell_id":"b50db6f2cb804932ae3f9e5748a6ea61","deepnote_cell_type":"markdown"},"source":["## 1.1 Generando un Baseline (0.5 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n","</p>\n","\n","Antes de entrenar un algoritmo, usted recuerda los apuntes de su mag칤ster en ciencia de datos y recuerda que debe seguir una serie de *buenas pr치cticas* para entrenar correcta y debidamente su modelo. Despu칠s de un par de vueltas, llega a las siguientes tareas:\n","\n","1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad.\n","2. Implemente un `FunctionTransformer` para extraer el d칤a, mes y a침o de la variable `date`. Guarde estas variables en el formato categorical de pandas.\n","3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos num칠ricos y categ칩ricos. Use `OneHotEncoder` para las variables categ칩ricas.\n","4. Guarde los pasos anteriores en un `Pipeline`, dejando como 칰ltimo paso el regresor `DummyRegressor` para generar predicciones en base a promedios.\n","5. Entrene el pipeline anterior y reporte la m칠trica `mean_absolute_error` sobre los datos de validaci칩n. 쮺칩mo se interpreta esta m칠trica para el contexto del negocio?\n","6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los par치metros por default**. 쮺칩mo cambia el MAE al implementar este algoritmo? 쮼s mejor o peor que el `DummyRegressor`?\n","7. Guarde ambos modelos en un archivo .pkl (uno cada uno)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE con DummyRegressor: 13298.497767341096\n","Features after preprocessing: ['price', 'id', 'lat', 'long', 'pop', 'day_28', 'day_29', 'day_30', 'day_31', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016', 'year_2017', 'year_2018', 'city_Athens', 'city_Irakleion', 'city_Larisa', 'city_Patra', 'city_Thessaloniki', 'shop_shop_1', 'shop_shop_2', 'shop_shop_3', 'shop_shop_4', 'shop_shop_5', 'shop_shop_6', 'brand_adult-cola', 'brand_gazoza', 'brand_kinder-cola', 'brand_lemon-boost', 'brand_orange-power', 'container_can', 'container_glass', 'container_plastic', 'capacity_1.5lt', 'capacity_330ml', 'capacity_500ml']\n","MAE con XGBRegressor: 2424.366823499591\n"]},{"data":{"text/plain":["['model_xgb.pkl']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Importar librer칤as necesarias\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, MinMaxScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.dummy import DummyRegressor\n","from sklearn.metrics import mean_absolute_error\n","from xgboost import XGBRegressor\n","import joblib\n","\n","\n","ordered_columns = ['price'] + [col for col in df.columns if col != 'price']\n","df = df[ordered_columns]\n","\n","# Crear un FunctionTransformer para extraer d칤a, mes y a침o\n","def extract_date_parts(df):\n","    df['day'] = df['date'].dt.day.astype('category')\n","    df['month'] = df['date'].dt.month.astype('category')\n","    df['year'] = df['date'].dt.year.astype('category')\n","    df = df.drop(columns=['date'])\n","    # Crear una lista con 'price' como el primer elemento seguido por el resto de las columnas\n","    #ordered_columns = ['price'] + [col for col in df.columns if col != 'price']\n","    #df = df[ordered_columns]\n","    return df\n","\n","\n","\n","\n","\n","\n","def get_feature_names(column_transformer):\n","    \"\"\"Get feature names from a ColumnTransformer.\"\"\"\n","    output_features = []\n","\n","    for name, pipe, features in column_transformer.transformers_:\n","        # Process each transformer\n","        if name != 'remainder':\n","            if hasattr(pipe, 'get_feature_names_out'):\n","                # If the transformer has a get_feature_names_out method, use it\n","                feature_names = pipe.get_feature_names_out(features)\n","                output_features.extend(feature_names)\n","            else:\n","                # Otherwise, just append the feature names as is\n","                output_features.extend(features)\n","        else:\n","            # If the remainder transformer is used, handle accordingly\n","            remainder_features = [f for f in features if f not in output_features]\n","            output_features.extend(remainder_features)\n","\n","    return output_features\n","\n","\n","\n","# Separar en conjuntos de train, validation y test\n","train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=(1/3), random_state=42)\n","\n","\n","\n","#print(extract_date_parts(df).head())\n","\n","date_transformer = FunctionTransformer(extract_date_parts, validate=False)\n","\n","# Crear un ColumnTransformer para procesar los datos\n","#numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n","\n","numeric_features = df.select_dtypes(include=['int64', 'float64']).drop('quantity', axis=1).columns\n","\n","\n","# Lista inicial de caracter칤sticas categ칩ricas\n","categorical_features = ['day', 'month', 'year']\n","\n","# A침adir columnas del DataFrame que no son de tipo int64 o float64\n","categorical_features.extend(df.select_dtypes(exclude=['int64', 'float64']).columns)\n","\n","# Excluir la columna 'date'\n","categorical_features.remove('date')\n","\n","\n","'''\n","print(\"numeric_features\")\n","print(numeric_features)\n","print(\"categorical_features\")\n","print(categorical_features)\n","'''\n","# Creando un dataframe que solo contiene la columna 'quantity'\n","train_Y = train_df[['quantity']]\n","\n","# Creando otro dataframe que contiene todas las columnas excepto 'quantity'\n","train_X = train_df.drop(columns=['quantity'])\n","\n","#print(train_X.columns)\n","\n","# Creando un dataframe que solo contiene la columna 'quantity'\n","test_Y = test_df[['quantity']]\n","\n","# Creando otro dataframe que contiene todas las columnas excepto 'quantity'\n","test_X = test_df.drop(columns=['quantity'])\n","\n","# Creando un dataframe que solo contiene la columna 'quantity'\n","val_Y= val_df[['quantity']]\n","\n","# Creando otro dataframe que contiene todas las columnas excepto 'quantity'\n","val_X = val_df.drop(columns=['quantity'])\n","\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', MinMaxScaler(), numeric_features),\n","        ('cat', OneHotEncoder(sparse_output=False), categorical_features)\n","    ])\n","\n","'''\n","# Fit and transform the data\n","df_eda = preprocessor.fit_transform(df)\n","\n","# Extract feature names for categorical features transformed by OneHotEncoder\n","# If 'cat' is the name given to the OneHotEncoder step in your ColumnTransformer\n","cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out()\n","\n","print(cat_feature_names)\n","\n","# Concatenate all feature names (numeric + categorical)\n","all_feature_names = numeric_features + list(cat_feature_names)\n","\n","# Create a DataFrame with the new feature names\n","df_eda = pd.DataFrame(df_eda, columns=all_feature_names)\n","\n","# Now you can use the corr() method\n","print(df_eda.corr())\n","'''\n","\n","# Crear y entrenar el pipeline con DummyRegressor\n","pipeline_dummy = Pipeline(steps=[\n","    ('date', date_transformer),\n","    ('preprocessor', preprocessor),\n","    ('regressor', DummyRegressor(strategy='mean'))\n","])\n","\n","\n","#print(\"debug1\")\n","pipeline_dummy.fit(train_X, train_Y) \n","\n","# Evaluar el modelo\n","y_pred = pipeline_dummy.predict(val_X)\n","\n","mae_dummy = mean_absolute_error(val_Y, y_pred)\n","print(f'MAE con DummyRegressor: {mae_dummy}')\n","\n","\n","# After fitting your pipeline, call this function\n","feature_names_after_preprocessing = get_feature_names(pipeline_dummy.named_steps['preprocessor'])\n","print(\"Features after preprocessing:\", feature_names_after_preprocessing)\n","\n","\n","# Reemplazar DummyRegressor con XGBRegressor y entrenar nuevamente\n","pipeline_xgb = Pipeline(steps=[\n","    ('date', date_transformer),\n","    ('preprocessor', preprocessor),\n","    ('regressor', XGBRegressor())\n","])\n","\n","pipeline_xgb.fit(train_X, train_Y) \n","\n","# Evaluar el nuevo modelo\n","y_pred_xgb = pipeline_xgb.predict(val_X)\n","mae_xgb = mean_absolute_error(val_Y, y_pred_xgb)\n","print(f'MAE con XGBRegressor: {mae_xgb}')\n","\n","# Guardar los modelos\n","joblib.dump(pipeline_dummy, 'model_dummy.pkl')\n","joblib.dump(pipeline_xgb, 'model_xgb.pkl')\n"]},{"cell_type":"markdown","metadata":{"cell_id":"7e17e46063774ec28226fe300d42ffe0","deepnote_cell_type":"markdown"},"source":["## 1.2 Forzando relaciones entre par치metros con XGBoost (1.0 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n","</p>\n","\n","Un colega aficionado a la econom칤a le *sopla* que la demanda guarda una relaci칩n inversa con el precio del producto. Motivado para impresionar al querido corp칩reo, se propone hacer uso de esta informaci칩n para mejorar su modelo.\n","\n","Vuelva a entrenar el `Pipeline`, pero esta vez forzando una relaci칩n mon칩tona negativa entre el precio y la cantidad. Luego, vuelva a reportar el `MAE` sobre el conjunto de validaci칩n. 쮺칩mo cambia el error al incluir esta relaci칩n? 쯊en칤a raz칩n su amigo?\n","\n","Nuevamente, guarde su modelo en un archivo .pkl\n","\n","Nota: Para realizar esta parte, debe apoyarse en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentaci칩n</a>.\n","\n","Hint: Para implementar el constraint, se le sugiere hacerlo especificando el nombre de la variable. De ser as칤, probablemente le sea 칰til **mantener el formato de pandas** antes del step de entrenamiento."]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"f469f3b572be434191d2d5c3f11b20d2","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE con XGBRegressor y constraint mon칩tono: 2500.521823322749\n"]},{"data":{"text/plain":["['model_xgb_monotone.pkl']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Definir los constraints de monoton칤a\n","# Asumiendo que 'price' es la primera columna despu칠s del preprocesamiento\n","# -1 indica una relaci칩n mon칩tona negativa\n","monotone_constraints = (-1,)\n","\n","# Incluir el inspector de datos en el pipeline antes del regresor\n","pipeline_xgb_monotone = Pipeline(steps=[\n","    ('date', date_transformer),\n","    ('preprocessor', preprocessor),\n","    ('regressor', XGBRegressor(monotone_constraints=monotone_constraints))\n","])\n","\n","pipeline_xgb_monotone.fit(train_X, train_Y)\n","\n","# Evaluar el modelo\n","y_pred_xgb_monotone = pipeline_xgb_monotone.predict(val_X)\n","mae_xgb_monotone = mean_absolute_error(val_Y, y_pred_xgb_monotone)\n","print(f'MAE con XGBRegressor y constraint mon칩tono: {mae_xgb_monotone}')\n","\n","# Guardar el modelo\n","joblib.dump(pipeline_xgb_monotone, 'model_xgb_monotone.pkl')\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"e59ef80ed20b4de8921f24da74e87374","deepnote_cell_type":"markdown"},"source":["## 1.3 Optimizaci칩n de Hiperpar치metros con Optuna (2.0 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n","</p>\n","\n","Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun m치s* su modelo. En particular, le comenta de la optimizaci칩n de hiperpar치metros con metodolog칤as bayesianas a trav칠s del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n","\n","A partir de la mejor configuraci칩n obtenida en la secci칩n anterior, utilice `optuna` para optimizar sus hiperpar치metros. En particular, se le pide:\n","\n","- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n","- Utilice `TPESampler` como m칠todo de muestreo\n","- De `XGBRegressor`, optimice los siguientes hiperpar치metros:\n","    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n","    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n","    - `max_depth` buscando valores enteros en el rango (3, 10)\n","    - `max_leaves` buscando valores enteros en el rango (0, 100)\n","    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n","    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n","    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n","- De `OneHotEncoder`, optimice el hiperpar치metro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n","- Explique cada hiperpar치metro y su rol en el modelo. 쮿acen sentido los rangos de optimizaci칩n indicados?\n","- Fije el tiempo de entrenamiento a 5 minutos\n","- Reportar el n칰mero de *trials*, el `MAE` y los mejores hiperpar치metros encontrados. 쮺칩mo cambian sus resultados con respecto a la secci칩n anterior? 쮸 qu칠 se puede deber esto?\n","- Guardar su modelo en un archivo .pkl"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"de5914621cc64cb0b1bacb9ff565a97e","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["N칰mero de trials: 100\n","Mejores hiperpar치metros: {'learning_rate': 0.07030656263631437, 'n_estimators': 829, 'max_depth': 9, 'max_leaves': 89, 'min_child_weight': 3, 'reg_alpha': 0.07298575769548347, 'reg_lambda': 0.7014716927898965, 'min_frequency': 0.6799054775385127}\n","MAE 칩ptimo: 2034.7008926934\n"]}],"source":["import optuna\n","\n","# Define la funci칩n objetivo para Optuna\n","def objective(trial):\n","    # Definir los rangos de b칰squeda para los hiperpar치metros\n","    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n","    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n","    max_depth = trial.suggest_int('max_depth', 3, 10)\n","    max_leaves = trial.suggest_int('max_leaves', 0, 100)\n","    min_child_weight = trial.suggest_int('min_child_weight', 1, 5)\n","    reg_alpha = trial.suggest_float('reg_alpha', 0, 1)\n","    reg_lambda = trial.suggest_float('reg_lambda', 0, 1)\n","    \n","    min_frequency = trial.suggest_float('min_frequency', 0.0, 1.0)\n","\n","    # Crear un nuevo pipeline con los hiperpar치metros sugeridos por Optuna\n","    xgb_model = XGBRegressor(\n","        learning_rate=learning_rate,\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        max_leaves=max_leaves,\n","        min_child_weight=min_child_weight,\n","        reg_alpha=reg_alpha,\n","        reg_lambda=reg_lambda\n","    )\n","\n","    # Modificar el valor de min_frequency del OneHotEncoder en el ColumnTransformer\n","    for name, transformer, columns in preprocessor.transformers_:\n","        if isinstance(transformer, OneHotEncoder):\n","            transformer.set_params(min_frequency=min_frequency)\n","\n","    pipeline_xgb = Pipeline(steps=[\n","        ('date', date_transformer),\n","        ('preprocessor', preprocessor),\n","        ('regressor', xgb_model)\n","    ])\n","\n","    pipeline_xgb.fit(train_X, train_Y)\n","\n","    # Calcular MAE en datos de validaci칩n\n","    y_pred = pipeline_xgb.predict(val_X)\n","    mae = mean_absolute_error(val_Y, y_pred)\n","\n","    return mae\n","\n","# Crear un estudio de Optuna\n","study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=314159))\n","study.optimize(objective, n_trials=100, timeout=300)\n","\n","# Obtener los mejores hiperpar치metros encontrados\n","best_params = study.best_params\n","mae_optuna_1 = study.best_value\n","num_trials = len(study.trials)\n","\n","print(\"N칰mero de trials:\", num_trials)\n","print(\"Mejores hiperpar치metros:\", best_params)\n","print(\"MAE 칩ptimo:\", mae_optuna_1)"]},{"cell_type":"markdown","metadata":{"cell_id":"5195ccfc37e044ad9453f6eb2754f631","deepnote_cell_type":"markdown"},"source":["## 1.4 Optimizaci칩n de Hiperpar치metros con Optuna y Prunners (1.7)\n","\n","<p align=\"center\">\n","  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n","</p>\n","\n","Despu칠s de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en s칤 mismo. Despu칠s de leer un par de post de personas de dudosa reputaci칩n en la *deepweb*, usted llega a la conclusi칩n que puede cumplir este objetivo mediante la implementaci칩n de **Prunning**.\n","\n","Vuelva a optimizar los mismos hiperpar치metros que la secci칩n pasada, pero esta vez utilizando **Prunning** en la optimizaci칩n. En particular, usted debe:\n","\n","- Responder: 쯈u칠 es prunning? 쮻e qu칠 forma deber칤a impactar en el entrenamiento?\n","- Utilizar `optuna.integration.XGBoostPruningCallback` como m칠todo de **Prunning**\n","- Fijar nuevamente el tiempo de entrenamiento a 5 minutos\n","- Reportar el n칰mero de *trials*, el `MAE` y los mejores hiperpar치metros encontrados. 쮺칩mo cambian sus resultados con respecto a la secci칩n anterior? 쮸 qu칠 se puede deber esto?\n","- Guardar su modelo en un archivo .pkl\n","\n","Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n","\n","```\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","```\n","\n","De implementar la opci칩n anterior, pueden especificar `show_progress_bar = True` en el m칠todo `optimize` para *m치s sabor*.\n","\n","Hint: Si quieren especificar par치metros del m칠todo .fit() del modelo a trav칠s del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n","\n","Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementaci칩n"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"eeaa967cd8f6426d8c54f276c17dce79","deepnote_cell_type":"code"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n","  warnings.warn(\n","/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `callbacks` in `fit` method is deprecated for better compatibility with scikit-learn, use `callbacks` in constructor or`set_params` instead.\n","  warnings.warn(\n","[W 2023-11-15 21:02:52,485] Trial 0 failed with parameters: {'learning_rate': 0.08197440749175473, 'n_estimators': 574, 'max_depth': 6, 'max_leaves': 9, 'min_child_weight': 5, 'reg_alpha': 0.9673564038996015, 'reg_lambda': 0.09820669376309132, 'min_frequency': 0.8018603712796477} because of the following error: KeyError('validation_0-mae').\n","Traceback (most recent call last):\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n","    value_or_values = func(trial)\n","                      ^^^^^^^^^^^\n","  File \"/tmp/ipykernel_88181/2405789120.py\", line 65, in objective_with_prunning\n","    xgb_model.fit(\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n","    return func(**kwargs)\n","           ^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1086, in fit\n","    self._Booster = train(\n","                    ^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n","    return func(**kwargs)\n","           ^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/training.py\", line 182, in train\n","    if cb_container.after_iteration(bst, i, dtrain, evals):\n","       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py\", line 241, in after_iteration\n","    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py\", line 241, in <genexpr>\n","    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/ignacio/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/integration/xgboost.py\", line 71, in after_iteration\n","    current_score = evaluation_results[self._observation_key]\n","                    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n","KeyError: 'validation_0-mae'\n","[W 2023-11-15 21:02:52,488] Trial 0 failed with value None.\n"]},{"ename":"KeyError","evalue":"'validation_0-mae'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mae\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m study_with_prunning \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m314159\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m study_with_prunning\u001b[39m.\u001b[39;49moptimize(objective_with_prunning, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Obtener los mejores hiperpar치metros encontrados con prunning\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m best_params_with_prunning \u001b[39m=\u001b[39m study_with_prunning\u001b[39m.\u001b[39mbest_params\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n","\u001b[1;32m/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mColumnas entregadas a XGBoost:\u001b[39m\u001b[39m\"\u001b[39m, train_X_processed\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# Entrenar el modelo con los datos procesados\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m xgb_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     train_X_processed, train_Y, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(val_X_processed, val_Y)], \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[XGBoostPruningCallback(trial, \u001b[39m\"\u001b[39;49m\u001b[39mvalidation_0-mae\u001b[39;49m\u001b[39m\"\u001b[39;49m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Calcular MAE en datos de validaci칩n\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ignacio/2023-2/mds/MDS7202/lab9/Enunciado.ipynb#X22sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m y_pred \u001b[39m=\u001b[39m xgb_model\u001b[39m.\u001b[39mpredict(val_X_processed)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m (\n\u001b[1;32m   1078\u001b[0m     model,\n\u001b[1;32m   1079\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1085\u001b[0m )\n\u001b[0;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1087\u001b[0m     params,\n\u001b[1;32m   1088\u001b[0m     train_dmatrix,\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39;49mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    185\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39mafter_training(bst)\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[39m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39mafter_iteration(model, epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/xgboost/callback.py:241\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m     metric_score \u001b[39m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 241\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39;49mafter_iteration(model, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n","File \u001b[0;32m~/miniconda3/envs/.proyecto1mds/lib/python3.11/site-packages/optuna/integration/xgboost.py:71\u001b[0m, in \u001b[0;36mXGBoostPruningCallback.after_iteration\u001b[0;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m             evaluation_results[key] \u001b[39m=\u001b[39m scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 71\u001b[0m current_score \u001b[39m=\u001b[39m evaluation_results[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_observation_key]\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trial\u001b[39m.\u001b[39mreport(current_score, step\u001b[39m=\u001b[39mepoch)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trial\u001b[39m.\u001b[39mshould_prune():\n","\u001b[0;31mKeyError\u001b[0m: 'validation_0-mae'"]}],"source":["import optuna\n","from optuna.integration import XGBoostPruningCallback\n","from sklearn.base import BaseEstimator, TransformerMixin\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","\n","\n","def objective_with_prunning(trial):\n","    # Definir los rangos de b칰squeda para los hiperpar치metros\n","    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n","    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n","    max_depth = trial.suggest_int('max_depth', 3, 10)\n","    max_leaves = trial.suggest_int('max_leaves', 0, 100)\n","    min_child_weight = trial.suggest_int('min_child_weight', 1, 5)\n","    reg_alpha = trial.suggest_float('reg_alpha', 0, 1)\n","    reg_lambda = trial.suggest_float('reg_lambda', 0, 1)\n","    min_frequency = trial.suggest_float('min_frequency', 0.0, 1.0)\n","\n","    # Crear el modelo XGBoost con los hiperpar치metros sugeridos\n","    xgb_model = XGBRegressor(\n","        learning_rate=learning_rate,\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        max_leaves=max_leaves,\n","        min_child_weight=min_child_weight,\n","        reg_alpha=reg_alpha,\n","        reg_lambda=reg_lambda,\n","        random_state=42  # Para garantizar la reproducibilidad\n","    )\n","\n","    '''\n","    # Modificar OneHotEncoder en el preprocesador\n","    for name, transformer, columns in preprocessor.transformers_:\n","        if isinstance(transformer, OneHotEncoder):\n","            transformer.set_params(min_frequency=min_frequency)\n","\n","    '''\n","\n","    class ColumnInspector(BaseEstimator, TransformerMixin):\n","        def fit(self, X, y=None):\n","            return self\n","\n","        def transform(self, X):\n","            # Si X es un DataFrame, imprime las columnas\n","            if hasattr(X, 'columns'):\n","                print(\"Columnas entregadas a XGBoost:\", X.columns)\n","            # Si no, asume que es un array numpy y no hace nada especial\n","            return X\n","\n","    # Crear el pipeline\n","    pipeline_xgb = Pipeline(steps=[\n","        ('preprocessor', preprocessor),\n","        ('regressor', xgb_model)\n","    ])\n","\n","    # Aqu칤 aplicamos el preprocesador manualmente para inspeccionar los datos\n","    train_X_processed = preprocessor.fit_transform(train_X)\n","    val_X_processed = preprocessor.transform(val_X)\n","\n","    # Imprimir las columnas despu칠s del preprocesamiento\n","    # Esto asume que el resultado es un DataFrame, si no lo es, ajustar seg칰n sea necesario\n","    if hasattr(train_X_processed, 'columns'):\n","        print(\"Columnas entregadas a XGBoost:\", train_X_processed.columns)\n","\n","    # Entrenar el modelo con los datos procesados\n","    xgb_model.fit(\n","        train_X_processed, train_Y, \n","        eval_set=[(val_X_processed, val_Y)], \n","        early_stopping_rounds=100, \n","        callbacks=[XGBoostPruningCallback(trial, \"validation_0-mae\")]\n","    )\n","\n","    # Calcular MAE en datos de validaci칩n\n","    y_pred = xgb_model.predict(val_X_processed)\n","    mae = mean_absolute_error(val_Y, y_pred)\n","\n","    return mae\n","\n","\n","study_with_prunning = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=314159))\n","study_with_prunning.optimize(objective_with_prunning, n_trials=100, timeout=300)\n","\n","# Obtener los mejores hiperpar치metros encontrados con prunning\n","best_params_with_prunning = study_with_prunning.best_params\n","mae_optuna_with_prunning = study_with_prunning.best_value\n","num_trials_with_prunning = len(study_with_prunning.trials)\n","\n","print(\"N칰mero de trials con prunning:\", num_trials_with_prunning)\n","print(\"Mejores hiperpar치metros con prunning:\", best_params_with_prunning)\n","print(\"MAE 칩ptimo con prunning:\", mae_optuna_with_prunning)"]},{"cell_type":"markdown","metadata":{"cell_id":"8a081778cc704fc6bed05393a5419327","deepnote_cell_type":"markdown"},"source":["## 1.5 Visualizaciones (0.5 puntos)\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n","</p>\n","\n","\n","Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n","\n","A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n","\n","- Gr치fico de historial de optimizaci칩n\n","- Gr치fico de coordenadas paralelas\n","- Gr치fico de importancia de hiperpar치metros\n","\n","Comente sus resultados: 쮻esde qu칠 *trial* se empiezan a observar mejoras notables en sus resultados? 쯈u칠 tendencias puede observar a partir del gr치fico de coordenadas paralelas? 쮺u치les son los hiperpar치metros con mayor importancia para la optimizaci칩n de su modelo?"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"0e706dc9a8d946eda7a9eb1f0463c6d7","deepnote_cell_type":"code"},"outputs":[],"source":["# Inserte su c칩digo ac치"]},{"cell_type":"markdown","metadata":{"cell_id":"ac8a20f445d045a3becf1a518d410a7d","deepnote_cell_type":"markdown"},"source":["## 1.6 S칤ntesis de resultados (0.3)\n","\n","Finalmente, genere una tabla resumen del MAE obtenido en los 5 modelos entrenados (desde Baseline hasta XGBoost con Constraints, Optuna y Prunning) y compare sus resultados. 쯈u칠 modelo obtiene el mejor rendimiento? \n","\n","Por 칰ltimo, cargue el mejor modelo, prediga sobre el conjunto de test y reporte su MAE. 쮼xisten diferencias con respecto a las m칠tricas obtenidas en el conjunto de validaci칩n? 쯇orqu칠 puede ocurrir esto?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calcula el MAE para cada modelo\n","mae_baseline = mae_dummy\n","mae_xgboost = mae_xgb  \n","mae_optuna = mae_optuna_1\n","mae_constraints = mae_xgb_monotone \n","mae_pruning = 3.5  \n","\n","# Crea un DataFrame con los valores de MAE\n","data = {\n","    'Modelo': ['Baseline', 'XGBoost', 'XGBoost (Optuna)', 'XGBoost (Constraints)', 'XGBoost (Optuna, Pruning)'],\n","    'MAE': [mae_baseline, mae_xgboost, mae_optuna, mae_constraints, mae_pruning]\n","}\n","\n","tabla = pd.DataFrame(data)\n","\n","# Imprime la tabla resumen\n","display(tabla)"]},{"cell_type":"markdown","metadata":{"cell_id":"5c4654d12037494fbd385b4dc6bd1059","deepnote_cell_type":"markdown"},"source":["# Conclusi칩n\n","Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/8CT1AXElF_cAAAAC/gojo-satoru.gif\">\n","</p>"]},{"cell_type":"markdown","metadata":{"cell_id":"5025de06759f4903a26916c80323bf25","deepnote_cell_type":"markdown"},"source":[]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"f63d38450a6b464c9bb6385cf11db4d9","deepnote_persisted_session":{"createdAt":"2023-11-09T16:18:30.203Z"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
